# hkc_basic_info_to_original.py
import re
import time
import requests
from bs4 import BeautifulSoup, NavigableString, Tag
from urllib.parse import urlparse, urljoin
import urllib.robotparser as robotparser
from datetime import datetime, timezone
from typing import Dict, Optional, Iterable

from pymongo import MongoClient
from config import mongoUri

# -------------------- Config --------------------
BASE = "https://hongkong-corp.com"
USER_AGENT = "learning-scraper/0.1 (+your-email@example.com)"
TIMEOUT = 20
DELAY_SECONDS = 1.0  # polite pause between requests

# ðŸ‘‰ Set this to the *company detail* page you want to fetch
TARGET_URL = "https://hongkong-corp.com/co/agarwood-technology-limited"
# Optional name to store alongside (falls back to parsed Business Name if None)
TARGET_NAME = None
# ------------------------------------------------


# ---------- Polite HTTP ----------
def allowed_to_fetch(base: str, user_agent: str, path: str) -> bool:
    robots_url = urljoin(base, "/robots.txt")
    rp = robotparser.RobotFileParser()
    try:
        rp.set_url(robots_url)
        rp.read()
        return rp.can_fetch(user_agent, path)
    except Exception:
        print(f"Warning: could not read robots.txt at {robots_url}. Aborting by default.")
        return False


def fetch_html(url: str, session: requests.Session, timeout: int = TIMEOUT) -> str:
    resp = session.get(url, timeout=timeout)
    resp.raise_for_status()
    return resp.text


# ---------- Basic Information parsing ----------
BASIC_INFO_SECTION_TITLES = [
    "Basic Info", "Basic Information", "Basic information",
    "åŸºæœ¬ä¿¡æ¯", "åŸºæœ¬è³‡æ–™", "å…¬å¸åŸºæœ¬è³‡æ–™", "å…¬å¸è³‡æ–™"
]

CANON_KEYS = [
    "brNo",
    "Business Name",
    "Business Name(Chinese)",
    "Registration Date",
    "Business Status",
    "Business Type",
    "Remarks",
    "Winding Up Mode",
    "Date of Dissolution / Ceasing to Exist",
    "Register of Charges",
    "Important Note",
]

LABEL_MAP = {
    r"\b(BR|CR)\s*No\.?\b": "brNo",
    r"Business\s*Registration\s*(?:No|Number)\b": "brNo",
    r"å…¬å¸(?:è¨»å†Š|ç™»è¨˜)?ç·¨è™Ÿ|å•†æ¥­ç™»è¨˜(?:è™Ÿ|è™Ÿç¢¼)": "brNo",

    r"^Business\s*Name\b": "Business Name",
    r"^å…¬å¸åç¨±$": "Business Name",

    r"Business\s*Name\s*\(Chinese\)": "Business Name(Chinese)",
    r"å…¬å¸åç¨±ï¼ˆ?ä¸­æ–‡ï¼‰?$|ä¸­æ–‡åç¨±|å…¬å¸å…¶ä»–åç¨±": "Business Name(Chinese)",

    r"^Registration\s*Date\b": "Registration Date",
    r"è¨»å†Šæ—¥æœŸ|æˆç«‹æ—¥æœŸ": "Registration Date",

    r"^Business\s*Status\b": "Business Status",
    r"å…¬å¸ç‹€æ…‹|ç¾æ³|å…¬å¸ç¾ç‹€|å…¬å¸ç‹€æ…‹ç‚º|å…¬å¸çŽ°å†µ": "Business Status",

    r"^Business\s*Type\b": "Business Type",
    r"å…¬å¸é¡žåˆ¥|å…¬å¸é¡žåž‹|å…¬å¸ç±»åˆ«|å…¬å¸ç±»åž‹": "Business Type",

    r"^Remarks?$": "Remarks",
    r"å‚™è¨»|å¤‡æ³¨": "Remarks",

    r"^Winding\s*Up\s*Mode\b": "Winding Up Mode",
    r"æ¸…ç›¤æ¨¡å¼|æ¸…ç›˜æ¨¡å¼": "Winding Up Mode",

    r"Date\s*of\s*Dissolution\s*/\s*Ceasing\s*to\s*Exist": "Date of Dissolution / Ceasing to Exist",
    r"å·²å‘Šè§£æ•£.*æ—¥æœŸ|ä¸å†æ˜¯.*æ—¥æœŸ|è§£æ•£.*æ—¥æœŸ|è¨»éŠ·.*æ—¥æœŸ|æ³¨é”€.*æ—¥æœŸ": "Date of Dissolution / Ceasing to Exist",

    r"^Register\s*of\s*Charges\b": "Register of Charges",
    r"æŠ¼è¨˜ç™»è¨˜å†Š|æŠ¼è®°ç™»è®°å†Œ": "Register of Charges",

    r"^Important\s*Note\b": "Important Note",
    r"é‡è¦æç¤º|é‡è¦å‚™è¨»|é‡è¦äº‹é …|é‡è¦äº‹é¡¹": "Important Note",
}
LABEL_PATTERNS = [(re.compile(p, re.IGNORECASE), key) for p, key in LABEL_MAP.items()]

STOP_SECTION_TITLES = [
    "Name History", "æ›´ååŽ†å²", "æ–‡ä»¶ç´¢å¼•", "Document Index", "Comments", "Similar Names"
]

def _normalize_label_text(s: str) -> Optional[str]:
    s = " ".join(s.replace("\xa0", " ").split()).strip().strip(":ï¼š")
    for rx, key in LABEL_PATTERNS:
        if rx.search(s):
            return key
    return None

def _text(node: Optional[Tag]) -> str:
    if node is None:
        return ""
    return " ".join(node.get_text(" ", strip=True).replace("\xa0", " ").split())

def _clean_value(v: str) -> str:
    v = v.replace("\xa0", " ")
    v = re.sub(r"\s+", " ", v).strip()
    v = v.strip(")ï¼‰]ã€‘>ã€‚ï¼š:ã€ ")
    return v

def _normalize_br_no(v: str) -> str:
    if not v:
        return ""
    cleaned = v.strip().strip(")ï¼‰]ã€‘>ã€‚ï¼š:ã€ ")
    m = re.search(r"\b(\d{8})\b", cleaned)
    if m:
        return m.group(1)
    digits = re.sub(r"\D", "", cleaned)
    if len(digits) >= 8:
        return digits[:8]
    return cleaned

def _is_section_header(tag: Tag) -> bool:
    if tag.name in ["h1","h2","h3","h4","h5","h6"]:
        t = _text(tag)
        if any(st.lower() in t.lower() for st in STOP_SECTION_TITLES):
            return True
    return False

def _find_basic_info_header(soup: BeautifulSoup) -> Optional[Tag]:
    for hdr in soup.find_all(["h1","h2","h3","h4","h5","h6"]):
        t = _text(hdr)
        if any(title.lower() in t.lower() for title in BASIC_INFO_SECTION_TITLES):
            return hdr
    return None

def _next_value_after(tag: Tag) -> str:
    """
    From a label tag, walk forward to find the first non-empty text block
    that is NOT another label or a major section header.
    """
    cur = tag
    for nxt in cur.find_all_next():
        if nxt is cur:
            continue
        if isinstance(nxt, Tag):
            if _is_section_header(nxt):
                break  # stop at next big section
            t = _text(nxt)
            if not t:
                continue
            # skip if it's clearly another label (english/chinese echo)
            if _normalize_label_text(t):
                continue
            return _clean_value(t)
        elif isinstance(nxt, NavigableString):
            s = str(nxt).strip()
            if s:
                return _clean_value(s)
    return ""

def parse_basic_info_from_html(html: str) -> Dict[str, str]:
    soup = BeautifulSoup(html, "html.parser")
    data: Dict[str, str] = {k: "" for k in CANON_KEYS}

    # Find the Basic Info header and parse forward
    header = _find_basic_info_header(soup)
    search_root = header if header is not None else soup

    # 1) Table/DL quick paths (if present)
    table = search_root.find_next("table") if header else soup.find("table")
    if table:
        for tr in table.find_all("tr"):
            cells = tr.find_all(["th","td"])
            if len(cells) >= 2:
                key = _normalize_label_text(_text(cells[0]))
                if key:
                    data[key] = _clean_value(_text(cells[1]))
    dl = search_root.find_next("dl") if header else soup.find("dl")
    if dl:
        for dt in dl.find_all("dt"):
            dd = dt.find_next_sibling("dd")
            if not dd:
                continue
            key = _normalize_label_text(_text(dt))
            if key:
                data[key] = _clean_value(_text(dd))

    # 2) Label-then-next-block pattern (dominant on this site)
    #    Scan forward from the Basic Info header
    scan_start = header if header else soup
    for tag in scan_start.find_all_next(True):
        if header and _is_section_header(tag) and tag is not header:
            # Stop once we hit next major section (e.g., Name History)
            break
        t = _text(tag)
        key = _normalize_label_text(t)
        if not key:
            continue
        # find the value that follows this label
        val = _next_value_after(tag)
        if not val:
            continue
        # Business Type often has CN + EN on two consecutive blocks; prefer EN if present
        if key == "Business Type" and data[key]:
            # Keep an English-looking value if available
            if re.search(r"[A-Za-z]", val):
                data[key] = val
        else:
            data[key] = val

    # 3) Normalize BR No
    data["brNo"] = _normalize_br_no(data.get("brNo", ""))

    return data

def fetch_company_basic_info(session: requests.Session, url: str) -> Dict[str, str]:
    path = urlparse(url).path
    if not allowed_to_fetch(BASE, USER_AGENT, path):
        return {k: "" for k in CANON_KEYS}
    html = fetch_html(url, session, timeout=TIMEOUT)
    return parse_basic_info_from_html(html)


# ---------- Mongo wiring ----------
def ensure_indexes(db) -> None:
    db["Company_HK_Orginal"].create_index("url", unique=True)

def upsert_basic_info(db, url: str, name: Optional[str], info: Dict[str, str]):
    now = datetime.now(timezone.utc)
    doc = {
        "url": url,
        "name": name or info.get("Business Name", ""),
        **info,
        "updateTime": now,
    }
    db["Company_HK_Orginal"].update_one(
        {"url": url},
        {
            "$set": doc,
            "$setOnInsert": {"createTime": now},
        },
        upsert=True,
    )


# ---------- Main ----------
def main():
    if not TARGET_URL:
        raise SystemExit("Please set TARGET_URL to a company detail URL (e.g., https://hongkong-corp.com/co/xxxx).")

    # session
    session = requests.Session()
    session.headers.update({"User-Agent": USER_AGENT})

    # mongo
    client = MongoClient(mongoUri)
    db = client["test_db"]
    ensure_indexes(db)

    # fetch & parse
    print(f"Fetching Basic Information from: {TARGET_URL}")
    info = fetch_company_basic_info(session, TARGET_URL)

    # store
    upsert_basic_info(db, TARGET_URL, TARGET_NAME, info)

    # show result
    print("Saved to Company_HK_Orginal:")
    print(f"  url:  {TARGET_URL}")
    if TARGET_NAME:
        print(f"  name: {TARGET_NAME}")
    for k in CANON_KEYS:
        print(f"  {k}: {info.get(k, '')}")

    time.sleep(DELAY_SECONDS)

if __name__ == "__main__":
    main()
